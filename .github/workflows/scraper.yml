name: Run Daily GeM Scraper

on:
  schedule:
    # Runs every day at 19:00 UTC = 00:30 IST
    - cron: "0 19 * * *"
  workflow_dispatch:

jobs:
  run-scraper:
    runs-on: ubuntu-latest

    steps:
      # 1. Checkout repository
      - name: Checkout repository
        uses: actions/checkout@v4

      # 2. Set up Python
      - name: Set up Python 3.11
        uses: actions/setup-python@v4
        with:
          python-version: "3.11"

      # 3. Install dependencies
      - name: Install Python dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      # 4. Install Playwright browsers
      - name: Install Playwright browsers
        run: python -m playwright install --with-deps

      # 5. Run scraper and capture logs
      - name: Run GeM scraper
        env:
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}
          SUPABASE_BUCKET_NAME: gem-pdfs
          LOG_LEVEL: INFO
        run: |
          python gem-scraper/daily_gem_pdf_scraper.py > scraper.log 2>&1 || true

      # 6. Upload success JSON to Supabase
      - name: Upload success JSON to Supabase
        if: success()  # only if the scraper ran without crashing
        env:
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}
        run: |
          JSON_FILE=$(ls gem-scraper/daily_data/*.json | head -n 1 || true)
          if [ -n "$JSON_FILE" ]; then
            FILE_NAME=$(basename "$JSON_FILE")
            curl -X POST \
              "${SUPABASE_URL}/storage/v1/object/daily_meta/${FILE_NAME}" \
              -H "Authorization: Bearer ${SUPABASE_SERVICE_ROLE_KEY}" \
              -H "Content-Type: application/json" \
              --data-binary @"$JSON_FILE"
          fi

      # 7. Upload failure HTML + PNG snapshots (only on failure)
      - name: Upload failure snapshots to Supabase
        if: failure()
        env:
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}
        run: |
          for FILE in gem-scraper/daily_data/failures/*; do
            [ -e "$FILE" ] || continue
            NAME=$(basename "$FILE")
            curl -X POST \
              "${SUPABASE_URL}/storage/v1/object/failures/${NAME}" \
              -H "Authorization: Bearer ${SUPABASE_SERVICE_ROLE_KEY}" \
              -H "Content-Type: application/octet-stream" \
              --data-binary @"$FILE"
          done

      # 8. Upload scraper log (always)
      - name: Upload scraper log to Supabase
        if: always()
        env:
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}
        run: |
          LOG_NAME="scraper-$(date +'%Y-%m-%d').log"
          curl -X POST \
            "${SUPABASE_URL}/storage/v1/object/logs/${LOG_NAME}" \
            -H "Authorization: Bearer ${SUPABASE_SERVICE_ROLE_KEY}" \
            -H "Content-Type: text/plain" \
            --data-binary @"scraper.log"
